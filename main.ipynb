{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoonkim313/dataCampusProject-Team10/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "obGBWFLnIypX",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {}
      },
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/mnt')\n",
        "nb_path = '/content/notebooks'\n",
        "os.symlink('/content/mnt/My Drive/Colab Notebooks', nb_path)\n",
        "sys.path.insert(0, nb_path)\n",
        "from konlpy.tag import Hannanum, Okt\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import heapq\n",
        "import pandas as pd\n",
        "from operator import itemgetter\n",
        "from collections import deque\n",
        "from ast import literal_eval\n",
        "from collections import defaultdict\n",
        "from soykeyword.lasso import LassoKeywordExtractor\n",
        "from pprint import pprint\n",
        "from krwordrank.word import KRWordRank\n",
        "from copy import deepcopy\n",
        "import kss\n",
        "import itertools\n",
        "import unicodedata\n",
        "import requests\n",
        "from functools import reduce\n",
        "from transformers import *\n",
        "import torch\n",
        "from summarizer import Summarizer\n",
        "from bs4 import BeautifulSoup\n",
        "import string\n",
        "from textrankr import TextRank\n",
        "from lexrankr import LexRank\n",
        "\n",
        "%cd /content/mnt/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT\n",
        "import frame_parser\n",
        "path=\"/content/mnt/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\"\n",
        "parser = frame_parser.FrameParser(model_path=path, language='ko')\n",
        "h = Hannanum()\n",
        "okt = Okt()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJMQe0p-PO01",
        "colab_type": "text"
      },
      "source": [
        "### Flask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8JWigx7M9UY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "f0df9a95-5518-400a-a315-bd20d8cfee1c"
      },
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return \"<h1>Running Flask on Google Colab!</h1>\"\n",
        "app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:werkzeug: * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://aefb9f12ce4e.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar-s1XyrwKmj",
        "colab_type": "text"
      },
      "source": [
        "#### Linked List êµ¬í˜„í•´ì„œ Frame Netì˜ ì˜ë¯¸ì—­ parsing í›„ë³´êµ°ì— ì‚¬ìš©"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gn8ClK-v4uo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Node:\n",
        "    def __init__(self, data):\n",
        "        self.words = data[0]\n",
        "        self.tags = data[3]\n",
        "        self.next = None\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str((self.words, self.tags))\n",
        "\n",
        "class LinkedList:\n",
        "  def __init__(self):\n",
        "      self.head = None\n",
        "\n",
        "  def __repr__(self):\n",
        "      node = self.head\n",
        "      nodes = []\n",
        "      while node is not None:\n",
        "          nodes.append(str(node.tags))\n",
        "          node = node.next\n",
        "      nodes.append(\"None\")\n",
        "      return ' -> '.join(nodes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl_ThLJhwOS4",
        "colab_type": "text"
      },
      "source": [
        "### Multiple Inheritance by Super()\n",
        "\n",
        "  **Text --> Highlight --> Relation**\n",
        "\n",
        "  **Text --> Highlight --> Summarize**\n",
        "\n",
        "\n",
        "\n",
        "Text í´ë˜ìŠ¤\n",
        "\n",
        "    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ë‹¨ë³„ë¡œ, ë¬¸ì¥ë³„ë¡œ ë‚˜ëˆ„ì–´ì¤Œ\n",
        "\n",
        "Highlight í´ë˜ìŠ¤\n",
        "     \n",
        "     CSS ë¬¸ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì¤‘ì‹¬ë¬¸ì¥ì—ëŠ” underline, ì¤‘ì‹¬ ë‹¨ì–´ì—ëŠ” highlight, ê´€ê³„ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ë‹¨ì–´ë“¤ì—ëŠ” boxë¥¼ ì‚½ì…í•´ì¤€ë‹¤\n",
        "\n",
        "Relation í´ë˜ìŠ¤\n",
        "\n",
        "    frameNETì˜ 807ê°œì˜ ì˜ë¯¸ì—­ì„ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ë“¤ì˜ ë‹¤ì˜ì„±ì„ ê³ ë ¤í•œ ìš©ë¡€, beginning/ inside/ outside taggingì„ ì´ìš©í•œë‹¤. êµ¬ì ˆ ì‚¬ì´ì˜ ì–¸ì–´ì  ê´€ê³„ì„±ì„ íŒŒì•…í•  ìˆ˜ ìˆìœ¼ë©° ì´ë¥¼ í†µí•´ ìš”ì•½, ì••ì¶•ì„ êµ¬í˜„í•¨.\n",
        "     \n",
        "Summarize í´ë˜ìŠ¤\n",
        "\n",
        "    Bert, Textrank, Lexrank ì„¸ê°€ì§€ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ ê²°ê³¼ë¥¼ multiple votingí•œë‹¤\n",
        "    ì¤‘ì‹¬ ë¬¸ì¥ì€ ê°€ì¥ ë†’ì€ ë“í‘œìˆ˜ë¥¼ ì°¨ì§€í•œ ë¬¸ì¥ìœ¼ë¡œ ë¦¬í„´í•¨\n",
        "  Relation.__mro__\n",
        "  \n",
        "  result : (__main__.Relation, __main__.Highlight, __main__.Text, object)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGCJPBUtvXj2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Text():\n",
        "    def __init__(self, text):\n",
        "        text = re.sub(\"'\", ' ', text)\n",
        "        paragraphs = text.split('\\n')\n",
        "        self.text = text\n",
        "        self.paragraphs = [i for i in paragraphs if i]\n",
        "        self.docs = [kss.split_sentences(paragraph) for paragraph in paragraphs if kss.split_sentences(paragraph)]\n",
        "        self.newtext = deepcopy(self.text)\n",
        "        print(\"TEXT\")\n",
        "\n",
        "    def findall(self, p, s):\n",
        "        i = s.find(p)\n",
        "        while i != -1:\n",
        "            yield i\n",
        "            i = s.find(p, i + 1)\n",
        "    \n",
        "    def _matchSentenceIndex(self, sentence):\n",
        "        indices = [idx for idx, sent  in enumerate(self.docs[1]) if sent.startswith(str(sentence[:5]))]\n",
        "        return indices\n",
        "    \n",
        "    def matchSentenceIndex(self, summarized, paragraph):\n",
        "        vote = [0]*len(paragraph)\n",
        "        for i in summarized:\n",
        "          for idx, sent in enumerate(self.docs[1]):\n",
        "            if sent.startswith(str(i[:5])):\n",
        "              vote[idx]+=1  \n",
        "        return vote\n",
        "\n",
        "\n",
        "class Highlight(Text):\n",
        "    def __init__(self, text, candidates=None):\n",
        "        super().__init__(text)\n",
        "        self.candidates = candidates\n",
        "        self.cand = candidates\n",
        "        print(\"Highlight\")\n",
        "\n",
        "    def add_tags(self, underline = False, highlight = True):\n",
        "        if self.cand == None:\n",
        "            conj = 'ê·¸ë¦¬ê³ , ê·¸ëŸ°ë°, ê·¸ëŸ¬ë‚˜, ê·¸ë˜ë„, ê·¸ë˜ì„œ, ë˜ëŠ”, ë°, ì¦‰, ê²Œë‹¤ê°€, ë”°ë¼ì„œ, ë•Œë¬¸ì—, ì•„ë‹ˆë©´, ì™œëƒí•˜ë©´, ë‹¨, ì˜¤íˆë ¤, ë¹„ë¡, ì˜ˆë¥¼ ë“¤ì–´, ë°˜ë©´ì—, í•˜ì§€ë§Œ, ê·¸ë ‡ë‹¤ë©´, ë°”ë¡œ, ì´ì— ëŒ€í•´'\n",
        "            conj = conj.replace(\"'\", \"\")\n",
        "            self.candidates = conj.split(\",\")\n",
        "            self.idx = [(i, i + len(candidate)) for candidate in self.candidates for i in\n",
        "                        self.findall(candidate, self.text)]\n",
        "        else:\n",
        "            self.idx = [(i, i + len(candidate)) for candidate in self.candidates for i in\n",
        "                        self.findall(candidate, self.newtext)]\n",
        "\n",
        "        for i in range(len(self.idx)):\n",
        "            try:\n",
        "                if highligh:\n",
        "                    self.idx = [(start, start + len(candidate)) for candidate in self.candidates for start in\n",
        "                                self.findall(candidate, self.newtext)]\n",
        "                    word = self.newtext[self.idx[i][0]:self.idx[i][1]]\n",
        "                    if highlight and self.cand == None:\n",
        "                        tagged = \" <mark style='background-color:#F9D877'>%s</mark>\" % (word)\n",
        "                    elif highlight:\n",
        "                        tagged = \" <mark style='background-color:#FFD0F2'>%s</mark>\" % (word)\n",
        "                if underline:\n",
        "                    self.idx = [(start, start + len(candidate)) for candidate in self.candidates for start in\n",
        "                                self.findall(candidate, self.newtext)]\n",
        "                    word = self.newtext[self.idx[i][0]:self.idx[i][1]]\n",
        "                    tagged = \"<u style='text-decoration:underline; text-decoration-color:#906fa8; font-weight: bold; text-decoration-style: wavy'>%s</u>\" % (word)\n",
        "                \n",
        "                self.newtext = tagged.join([self.newtext[:self.idx[i][0]], self.newtext[self.idx[i][1]:]])\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return '\\n'.join(self.newtext.split(\"\\n\"))\n",
        "\n",
        "\n",
        "class Relation(Highlight):\n",
        "\n",
        "    def __init__(self, text, candidates=None, num=20):\n",
        "        super().__init__(text, candidates)\n",
        "        wordrank_extractor = KRWordRank(min_count=4, max_length=10)\n",
        "        self.keywords, rank, graph = wordrank_extractor.extract(self.paragraphs, num_keywords=num)\n",
        "        self.path = \"/content/mnt/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\"\n",
        "        p = []\n",
        "        kw = []\n",
        "        for k, v in self.keywords.items():\n",
        "            p.append(okt.pos(k))\n",
        "            kw.append(k)\n",
        "        words = self.text.split(' ')\n",
        "        s = set()\n",
        "        keylist = [word for i in kw for word in words if i in word]\n",
        "        for i in keylist:\n",
        "            s.add(i)\n",
        "        p = [okt.pos(word) for word in s]\n",
        "        s = set()\n",
        "        for idx in range(len(p)):\n",
        "            ls = p[idx]\n",
        "            for j in range(len(ls)):\n",
        "                tag = ls[j][1]\n",
        "                word = ls[j][0]\n",
        "                if tag == \"Noun\":\n",
        "                    s.add(word)\n",
        "        self.keys = []\n",
        "        for temp in s:\n",
        "            self.keys.append(\" \" + temp)\n",
        "        print(\"KEYS: \", self.keys)\n",
        "\n",
        "    def frameParse(self, id):\n",
        "        parser = frame_parser.FrameParser(model_path=self.path, language='ko')\n",
        "        ps = parser.parser(self.docs[id], sent_id='1', result_format='conll')\n",
        "        return ps\n",
        "\n",
        "    def extractFrame(self):\n",
        "        self.final = {}\n",
        "        for paragraph in self.docs:\n",
        "            print(\"PARAGRAPH: \", self.docs)\n",
        "            for idx in range(len(paragraph)):\n",
        "                parsed = frameParse(paragraph[idx])  # candidates ìƒì„±\n",
        "                self.final.setdefault(idx, str)\n",
        "                parsedList = LinkedList()\n",
        "                for j in range(len(parsed)):\n",
        "                    parsed_candidate = parsed[j]\n",
        "                    new_node = Node(parsed_candidate)\n",
        "                    if j == 0:\n",
        "                        old_node = new_node\n",
        "                        parsedList.head = old_node\n",
        "                    elif j == len(parsed) - 1:\n",
        "                        old_node.next = new_node\n",
        "                        new_node.next = None\n",
        "                        print(idx, '  ', parsedList)\n",
        "                        self.final[idx] = parsedList\n",
        "                    else:\n",
        "                        old_node.next = new_node\n",
        "                        old_node = new_node\n",
        "\n",
        "    def findConsecutiveBIO(self, words, tag):\n",
        "        began = False\n",
        "        count = 1\n",
        "        self.que = deque(words)\n",
        "        for i in range(len(words)):\n",
        "            if tag[i] == 'O' and not began:\n",
        "                self.que.popleft()\n",
        "                began = False\n",
        "            if tag[i] == 'O' and began:\n",
        "                self.que.pop()\n",
        "                began = False\n",
        "            if tag[i].startswith('B'):\n",
        "                began = True\n",
        "            if began & tag[i].startswith('I'):\n",
        "                count += 1\n",
        "        return self.que\n",
        "\n",
        "    def extractRelation(self):\n",
        "        s = \"\"\n",
        "        self.res = [] * 20\n",
        "        for k, v in self.final.items():\n",
        "            a = v.head\n",
        "            try:\n",
        "                while a is not None:\n",
        "                    i = 0\n",
        "                    temp = \" \"\n",
        "                    if len(a.words) == len(a.tags):\n",
        "                        q = findConsecutiveBIO(a.words, a.tags)\n",
        "                        print(f\"The que {q}\")\n",
        "                    else:\n",
        "                        print(\"ERROR OCCURED\")\n",
        "\n",
        "                    count = len(a.words)\n",
        "                    for tag in a.tags:\n",
        "                        if tag.startswith(\"O\"):\n",
        "                            count -= 1\n",
        "                    if count > len(a.words) * 0.45:\n",
        "                        print(f\"The threshold is reached !!ğŸ˜ The count is {count}\")\n",
        "                        temp = \" \".join(q)\n",
        "                        words = a.words\n",
        "                        tags = a.tags\n",
        "                    else:\n",
        "                        temp = None\n",
        "                        words = None\n",
        "                        tags = None\n",
        "                    self.res[i] = (words, tag, temp)\n",
        "                    i+=1\n",
        "                    a = a.next\n",
        "            except:\n",
        "                pass\n",
        "        self.res = [j for j in self.res if j]\n",
        "\n",
        "    def roles(self):\n",
        "        for k, tup in self.res.items():\n",
        "            a, b, c = tup\n",
        "            roles = [0] * len(b)\n",
        "            words = [0] * len(b)\n",
        "            for i in range(len(b)):\n",
        "                try:\n",
        "                    roles[i] = b[i].split(\"-\")[1]\n",
        "                except:\n",
        "                    roles[i] = b[i].split(\"-\")[0]\n",
        "            print(roles)\n",
        "            for _ in roles:\n",
        "                pass\n",
        "\n",
        "class Summarize(Highlight):\n",
        "\n",
        "  def __init__(self, text, idx):\n",
        "      print(\"SUMMARIZE\")\n",
        "      super().__init__(text)\n",
        "      self.idx = idx\n",
        "      self.paragraph = self.paragraphs[idx]\n",
        "      url = \"https://api.smrzr.io/summarize?ratio=0.15\"\n",
        "      headers = {\n",
        "          'content-type': 'raw/text',\n",
        "          'origin': 'https://smrzr.io',\n",
        "          'referer': 'https://smrzr.io/',\n",
        "          'sec-fetch-dest': 'empty',\n",
        "          'sec-fetch-mode': 'cors',\n",
        "          'sec-fetch-site': 'same-site',\n",
        "          \"user-agent\": \"Mozilla/5.0\"\n",
        "      }\n",
        "      resp = requests.post(url, headers=headers, data=self.paragraph.encode('utf-8'))\n",
        "      assert resp.status_code == 200\n",
        "\n",
        "      summary = resp.json()['summary']\n",
        "      self.brSum = summary.split('\\n') # ë¦¬ìŠ¤íŠ¸ì˜ í˜•íƒœë¡œ extracted sentence ë°˜í™˜í•¨\n",
        "      tr = TextRank(self.paragraph)\n",
        "      t = tr.summarize(2)\n",
        "      self.trSum = kss.split_sentences(t)\n",
        "      lr = LexRank() \n",
        "      lr.summarize(self.paragraph)\n",
        "      self.lrSum = lr.probe()\n",
        "      \n",
        "\n",
        "  def summarizeSlow(self, ratio, model_name='Distil'):\n",
        "      global model\n",
        "      if model_name == \"Distil\":\n",
        "          model = DistilBertModel.from_pretrained('distilbert-base-multilingual-cased', output_hidden_states=True)\n",
        "          tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
        "      elif model_name == \"Bert\":\n",
        "          bmodel = AutoModel.from_pretrained('bert-base-multilingual-uncased', output_hidden_states=True)\n",
        "          configuration = bmodel.config\n",
        "          configuration.vocab_size = 32000\n",
        "          configuration.max_position_embeddings = 384\n",
        "          configuration.hidden_size = 1024\n",
        "          configuration.num_attention_heads = 16\n",
        "          configuration.output_hidden_states = True\n",
        "          model = BertModel.from_pretrained(\"/Users/yoonk/Downloads/large_v2.bin\", config=configuration)\n",
        "          tokenizer = BertTokenizer.from_pretrained(\"/Users/yoonk/Downloads/large_v2_32k_vocab.txt\")\n",
        "\n",
        "      summarizer = Summarizer(custom_model=model, custom_tokenizer=tokenizer)\n",
        "      url = 'https://kapi.kakao.com/v1/translation/translate'\n",
        "      before_lang = 'kr'\n",
        "      after_lang = 'en'\n",
        "      KEY = '18fea080a0db2ee0346353e8db4466e8'\n",
        "      header = {\n",
        "          \"Authorization\": 'KakaoAK {}'.format(KEY)\n",
        "      }\n",
        "\n",
        "      data = {\n",
        "          \"src_lang\": before_lang,\n",
        "          \"target_lang\": after_lang,\n",
        "          \"query\": text\n",
        "      }\n",
        "\n",
        "      response = requests.get(url, headers=header, params=data)\n",
        "      result = response.json()\n",
        "      translated = result['translated_text'][0]\n",
        "      result_eng = summarizer(translated, ratio=ratio)\n",
        "\n",
        "      before_lang = 'en'\n",
        "      after_lang = 'kr'\n",
        "      data = {\n",
        "          \"src_lang\": before_lang,\n",
        "          \"target_lang\": after_lang,\n",
        "          \"query\": ''.join(result_eng)\n",
        "      }\n",
        "      before_lang = 'en'\n",
        "      after_lang = 'kr'\n",
        "\n",
        "      response = requests.get(url, headers=header, params=data)\n",
        "      result_kor = response.json()\n",
        "      summarized = result_kor['translated_text'][0]\n",
        "      return summarized\n",
        "\n",
        "      \n",
        "  def ensembleSummarize(self):\n",
        "      lenSentences=len(self.docs[self.idx])\n",
        "      s = [0]*lenSentences\n",
        "      for idx in range(lenSentences):\n",
        "          \n",
        "          if self.paragraph[idx] in self.brSum:\n",
        "              s[idx]+=1\n",
        "              print(\"ë“±ì¥! \", self.paragraph[idx])\n",
        "          \n",
        "          if self.paragraph[idx] in self.lrSum:\n",
        "              s[idx]+=1\n",
        "              print(\"ë“±ì¥! \", self.paragraph[idx])\n",
        "          \n",
        "          if self.paragraph[idx] in self.trSum:\n",
        "              s[idx]+=1\n",
        "              print(\"ë“±ì¥! \", self.paragraph[idx])\n",
        "\n",
        "      print(\"Bert Summarization\",self.brSum)\n",
        "      print()\n",
        "      print(\"LexRank Summarization\",self.lrSum)\n",
        "      print()\n",
        "      print(\"TextRank Summarization\",self.trSum)\n",
        "      i = s.index(max(s))\n",
        "      extracted = self.docs[self.idx][i]\n",
        "      return extracted\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_v5o0el7I9ur",
        "colab": {}
      },
      "source": [
        "text = '''\n",
        "í‰ë“±ì€ ììœ ì™€ ë”ë¶ˆì–´ ê·¼ëŒ€ ì‚¬íšŒì˜ í•µì‹¬ ì´ë…ìœ¼ë¡œ ìë¦¬ ì¡ê³ ìˆë‹¤. ì¸ê°„ì€ ê°€ë ¹ ì¸ì¢…ì´ë‚˜ ì„±ë³„ê³¼ ìƒê´€ì—†ì´ ëˆ„êµ¬ë‚˜ í‰ë“±í•˜ë‹¤ê³  ìƒê°í•œë‹¤. ëª¨ë“  ì¸ê°„ì€ í‰ë“±í•˜ë‹¤ê³  ë§í•˜ëŠ”ë°, ì´ ë§ì€ ë¬´ìŠ¨ ëœ»ì¼ê¹Œ? ê·¸ë¦¬ê³  ê·¸ ê·¼ê±°ëŠ” ë¬´ì—‡ì¸ê°€? \n",
        "ì¼ë‹¨ ì´ ë§ì„ ëª¨ë“  ì¸ê°„ì„ ëª¨ë“  ì¸¡ë©´ì—ì„œ ë˜‘ê°™ì´ ëŒ€ìš°í•˜ëŠ” ì ˆëŒ€ì¹™ í‰ë“±ìœ¼ë¡œ ìƒê°í•˜ëŠ” ì´ëŠ” ì—†ë‹¤. ì¸ê°„ì€ ì €ë§ˆë‹¤ ë‹¤ë¥´ê²Œ ê°€ì§€ê³  ëŒ€ì–´ë‚œ ëŠ¥ë ¥ê³¼ ì†Œì§ˆì„ ë˜‘ê°™ê²Œ ë§Œë“¤ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì´ë‹¤. ì ˆëŒ€ì  í‰ë“±ì€ ê°œì¸ì˜ ê°œì„±ì´ëƒ ììœ¨ì„± ë“±ì˜ ê°€ì¹˜ì™€ ì¶©ëŒí•˜ê¸°ë„ í•œë‹¤. í‰ë“±ì— ëŒ€í•œ ìš”êµ¬ëŠ” ëª¨ë“  ë¶ˆí‰ë“±ì„ ì•…ìœ¼ë¡œ ë³´ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì¶©ë¶„í•œ ì´ìœ ê°€ ì œì‹œë˜ì§€ ì•Šì€ ë¶ˆí‰ë“±ì„ ì œê±°í•˜ëŠ” ë° ëª©í‘œë¥¼ ë‘ê³  ìˆë‹¤. â€˜ì´ìœ  ì—†ëŠ” ì°¨ë³„ ê¸ˆì§€â€™ë¼ëŠ” ì¡°ê±´ì  í‰ë“± ì›ì¹™ì€ ì°¨ë³„ ëŒ€ìš°ë¥¼ í•  ë•ŒëŠ” ì´ìœ ë¥¼ ì œì‹œí•  ê²ƒì„ ìš”êµ¬í•˜ê³  ìˆë‹¤. ì´ê²ƒì€ ì–´ë–¤ ì´ìœ ê°€ ì œì‹œëœë‹¤ë©´ íŠ¹ì •í•œ ë¶€ë¥˜ì— ì†í•˜ëŠ” ì‚¬ëŒë“¤ì—ê²ŒëŠ” í‰ë“±í•œ ëŒ€ìš°ë¥¼, ê·¸ ë¶€ë¥˜ì— ì†í•˜ì§€ ì•ŠëŠ” ì‚¬ëŒë“¤ì—ê²ŒëŠ” ì°¨ë³„ì € ëŒ€ìš°ë¥¼ í•˜ëŠ” ê²ƒì„ í—ˆìš©í•œë‹¤. ê·¸ë ‡ë‹¤ë©´ ì‚¬ëŒë“¤ì„ íŠ¹ì •í•œ ë¶€ë¥˜ë¡œ êµ¬ë¶„í•˜ëŠ” ê¸°ì¤€ì€ ë¬´ì—‡ì¸ê°€? ì´ê²ƒì€ ë°”ë¡œ í‰ë“±ì˜ ê·¼ê±°ì— ëŒ€í•œ ë¬¼ìŒì´ë‹¤.\n",
        "ê·¼ëŒ€ì˜ ì—¬ëŸ¬ ì¸ê¶Œ ì„ ì–¸ì— ë‚˜íƒ€ë‚œ í‰ë“± ê°œë…ì€ ê°œì¸ë“¤ ì‚¬ì´ì˜ í‰ë“±ì„±ì„ íƒ€ê³ ë‚œ ìì—°ì  ê¶Œë¦¬ë¡œ ê°„ì£¼í•˜ì˜€ë‹¤. í•˜ì§€ë§Œ ì´ëŸ¬í•œ ìì—°ê¶Œ ì´ë¡ ì€ ë¬´ì—‡ì´ ìì—°ì  ê¶Œë¦¬ì´ê³  ê¶Œë¦¬ì˜ ì¡´ì¬ê°€ ìëª…í•œ ì´ìœ ê°€ ë¬´ì—‡ì¸ì§€ ë“±ì˜ ë¬¸ì œì— ë¶€ë”ªíˆê²Œ ëœë‹¤. ê·¸ë˜ì„œ ë¡¤ìŠ¤ëŠ” ê¸°ì¡´ì˜ ìì—°ê¶Œ ì‚¬ìƒì— ì˜ì¡´í•˜ì§€ ì•ŠëŠ” ë°©ì‹œìœ¼ë¡œ ì¸ê°„ í‰ë“±ì˜ ê·¼ê±°ë¥¼ ë§ˆëŸ°í•˜ë ¤ê³  í•œë‹¤. ê·¸ëŠ” ì–´ë–¤ ê·œì¹™ì´ ê³µí‰í•˜ê³  ì¼ê´€ë˜ê²Œ ìš´ì˜ë˜ë©°, ê·¸ ê·œì¹™ì— ë”°ë¼ ìœ ì‚¬í•œ ê²½ìš°ëŠ” ìœ ì‚¬í•˜ê²Œ ì·¨ê¸‰ëœë‹¤ë©´ í˜•ì‹ì  ì •ì˜ëŠ” ì‹¤í˜„ëœë‹¤ê³  ë³¸ë‹¤. í•˜ì§€ë§Œ ë¡¤ìŠ¤ëŠ” í˜•ì‹ì € ì •ì˜ì— ë”°ë¼ ê·œì¹™ì„ ì¤€ìˆ˜í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œëŠ” ì •ì˜ë¥¼ ë‹´ë³´í•  ìˆ˜ ì—†ë‹¤ê³  ìƒê°í•œë‹¤. ê·¸ ê·œì¹™ì´ ë” ë†’ì€ ë„ë•ì  ê¶Œìœ„ë¥¼ ì§€ë‹Œ ë‹¤ë¥¸ ì´ë„˜ê³¼ ì¶©ëŒí•  ìˆ˜ ì—ˆê¸°ì—, ì‹¤ì§ˆì  ì •ì˜ê°€ ë³´ì¥ë˜ê¸° ìœ„í•´ì„œëŠ” ê·œì¹™ì˜ ë‚´ìš©ì´ ì¤‘ìš”í•œ ê²ƒì´ë‹¤.\n",
        "ë¡¤ìŠ¤ëŠ” ì¸ê°„ í‰ë“±ì˜ ê·¼ê±°ë¥¼ ì„¤ëª…í•˜ë©´ì„œ ì˜ì—­ ì„±ì§ˆ (range property) ê°œë…ì„ ë„ì…í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì–´ë–¤ ì›ì˜ ë‚´ë¶€ì— ìˆëŠ” ì ë“¤ì€ ê·¸ ìœ„ì¹˜ê°€ ì„œë¡œ ë‹¤ë¥´ì§€ë§Œ ì›ì˜ ë‚´ë¶€ì— ìˆë‹¤ëŠ” ì ì—ì„œ ë™ì¼í•œ ì˜ì—­ ì„±ì§ˆì„ ê°–ëŠ”ë‹¤. ë°˜ë©´ì— ì›ì˜ ë‚´ë¶€ì— ìˆëŠ” ì§‘ê³¼ ì›ì˜ ì™¸ë¶€ì— ì˜€ëŠ” ì ì€ì›ì˜ ê²½ê³„ì„ ì„ ê¸°ì¤€ìœ¼ë¡œ ì„œë¡œ ë‹¤ë¥¸ ì˜ì—­ ì„±ì§ˆì„ ê°–ëŠ”ë‹¤. ê·¸ëŠ” í‰ë“±í•œ ëŒ€ìš°ë¥¼ ë°›ê¸° ìœ„í•œ ì˜ì—­ ì„±ì§ˆë¡œì„œ â€˜ë„ë•ì  ì¸ê²©'ì„ ì œì‹œí•œë‹¤. ë„ë•ì  ì¸ê²©ì´ë€ ë„ë•ì  í˜¸ì†Œê°€ ê°€ëŠ¥í•˜ê³  ê·¸ëŸ° í˜¸ì†Œì— ê´€ì‹¬ì„ ê¸°ì´ëŠ” ëŠ¥ë ¥ì´ ìˆë‹¤ëŠ” ê²ƒì¸ë°, ì´ ëŠ¥ë ¥ì„ ìµœì†Œì¹˜ë§Œ ê°–ê³  ìˆë‹¤ë©´ í‰ë“±í•œ ëŒ€ìš°ì— ëŒ€í•œ ê¶Œí•œì„ ê°–ê²Œ ëœë‹¤. ë„ë•ì  ì¸ê²©ì´ë¼ê³  í•´ì„œ ë„ë•ì ìœ¼ë¡œ í›Œë¥­í•˜ë‹¤ëŠ” ëœ»ì´ ì•„ë‹ˆë¼ ë„ë•ê³¼ ë¬´ê´€í•˜ë‹¤ëŠ” ë§ê³¼ ëŒ€ë¹„ë˜ëŠ” ëœ»ìœ¼ë¡œ ì“°ê³  ìˆë‹¤. ê·¸ëŸ°ë° ì–´ë¦° ì•„ì´ëŠ” ì¸ê²©ì²´ë¡œì„œì˜ ìµœì†Œí•œì˜ ê¸°ì¤€ì„ ì¶©ì¡±í•˜ê³  ìˆëŠ”ì§€ê°€ ë…¼ë€ì´ ë  ìˆ˜ ìˆë‹¤. ì´ì— ëŒ€í•´ ë¡¤ìŠ¤ëŠ” ë„ë•ì  ì¸ê²©ì„ ê·œì •í•˜ëŠ” ìµœì†Œí•œì˜ ìš”êµ¬ ì¡°ê±´ì€ ì ì¬ì  ëŠ¥ë ¥ì´ì§€ ê·¸ê²ƒì˜ ì‹¤í˜„ ì—¬ë¶€ê°€ ì•„ë‹ˆê¸°ì— ì–´ë¦° ì•„ì´ë„ í‰ë“±í•œ ì¡´ì¬ë¼ê³  ë§í•œë‹¤. ì‹±ì–´ëŠ” ìœ„ì™€ ê°™ì€ ë¡¤ìŠ¤ì˜ ì‹œë„ë¥¼ ë¹„íŒí•œë‹¤. ë„ë•ì— ëŒ€í•œ ë¯¼ê°ì„±ì˜ ìˆ˜ì¤€ì€ ì‚¬ëŒì— ë”°ë¼ ë‹¤ë¥´ë‹¤. ê·¸ë˜ì„œ ë„ë•ì  ì¸ê²©ì˜ ëŠ¥ë ¥ì´ ê·¸ë ‡ê²Œ ì¤‘ìš”í•˜ë‹¤ë©´ ê·¸ê²ƒì„ ê°–ì¶˜ ì •ë„ì— ë”°ë¼ ë„ë•ì  ìœ„ê³„ë¥¼ ë‹¤ë¥´ê²Œ í•˜ì§€ ë§ì•„ì•¼ í•  ì´ìœ ê°€ ë¶„ëª…í•˜ì§€ ì•Šë‹¤ê³  ë§í•œë‹¤. \n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrwJKcTB-S8j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "ad275e66-de67-4b38-a02f-4e14dc065552"
      },
      "source": [
        "sm = Summarize(text,1)\n",
        "sm.ensembleSummarize()\n",
        "sm.matchSentenceIndex()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SUMMARIZE\n",
            "TEXT\n",
            "Highlight\n",
            "Bert Summarization ['ì¼ë‹¨ ì´ ë§ì„ ëª¨ë“  ì¸ê°„ì„ ëª¨ë“  ì¸¡ë©´ì—ì„œ ë˜‘ê°™ì´ ëŒ€ìš°í•˜ëŠ” ì ˆëŒ€ì¹™ í‰ë“±ìœ¼ë¡œ ìƒê°í•˜ëŠ” ì´ëŠ” ì—†ë‹¤ . ì¸ê°„ì€ ì €ë§ˆë‹¤ ë‹¤ë¥´ê²Œ ê°€ì§€ê³  ëŒ€ì–´ë‚œ ëŠ¥ë ¥ê³¼ ì†Œì§ˆì„ ë˜‘ê°™ê²Œ ë§Œë“¤ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì´ë‹¤ .']\n",
            "\n",
            "LexRank Summarization ['â€˜ì´ìœ  ì—†ëŠ” ì°¨ë³„ ê¸ˆì§€â€™ë¼ëŠ” ì¡°ê±´ì  í‰ë“± ì›ì¹™ì€ ì°¨ë³„ ëŒ€ìš°ë¥¼ í•  ë•ŒëŠ” ì´ìœ ë¥¼ ì œì‹œí•  ê²ƒì„ ìš”êµ¬í•˜ê³  ìˆë‹¤', 'ê·¸ë ‡ë‹¤ë©´ ì‚¬ëŒë“¤ì„ íŠ¹ì •í•œ ë¶€ë¥˜ë¡œ êµ¬ë¶„í•˜ëŠ” ê¸°ì¤€ì€ ë¬´ì—‡ì¸ê°€? ì´ê²ƒì€ ë°”ë¡œ í‰ë“±ì˜ ê·¼ê±°ì— ëŒ€í•œ ë¬¼ìŒì´ë‹¤']\n",
            "\n",
            "TextRank Summarization ['í‰ë“±ì— ëŒ€í•œ ìš”êµ¬ëŠ” ëª¨ë“  ë¶ˆí‰ë“±ì„ ì•…ìœ¼ë¡œ ë³´ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì¶©ë¶„í•œ ì´ìœ ê°€ ì œì‹œë˜ì§€ ì•Šì€ ë¶ˆí‰ë“±ì„ ì œê±°í•˜ëŠ” ë° ëª©í‘œë¥¼ ë‘ê³  ìˆë‹¤.', 'â€˜ì´ìœ  ì—†ëŠ” ì°¨ë³„ ê¸ˆì§€â€™ë¼ëŠ” ì¡°ê±´ì  í‰ë“± ì›ì¹™ì€ ì°¨ë³„ ëŒ€ìš°ë¥¼ í•  ë•ŒëŠ” ì´ìœ ë¥¼ ì œì‹œí•  ê²ƒì„ ìš”êµ¬í•˜ê³  ìˆë‹¤.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/notebooks/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
            "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ì¼ë‹¨ ì´ ë§ì„ ëª¨ë“  ì¸ê°„ì„ ëª¨ë“  ì¸¡ë©´ì—ì„œ ë˜‘ê°™ì´ ëŒ€ìš°í•˜ëŠ” ì ˆëŒ€ì¹™ í‰ë“±ìœ¼ë¡œ ìƒê°í•˜ëŠ” ì´ëŠ” ì—†ë‹¤.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud7wmTvCILR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp = 'â€˜ì´ìœ  ì—†ëŠ” ì°¨ë³„ ê¸ˆì§€â€™ë¼ëŠ” ì¡°ê±´ì  í‰ë“± ì›ì¹™ì€ ì°¨ë³„ ëŒ€ìš°ë¥¼ í•  ë•ŒëŠ” ì´ìœ ë¥¼ ì œì‹œí•  ê²ƒì„ ìš”êµ¬í•˜ê³  ìˆë‹¤'\n",
        "if temp in sm.docs[1]:\n",
        "  print(\"ë“±ì¥\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyEogB3pIWBd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ecce6c09-9a5b-47e0-b603-84f8cb64c794"
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S7zGTQO4BUO7",
        "colab": {}
      },
      "source": [
        "# 1st sentence : GOALS & MEANS \n",
        "\n",
        "parsed = parser.parser(ls[0], sent_id='1', result_format='conll')\n",
        "words = parsed[0][0]\n",
        "print(\"ğŸ˜€ Words vector from sentence \",words)\n",
        "q = [-1]*len(parsed)\n",
        "for i in range(len(parsed)):\n",
        "  count=0\n",
        "  for element in parsed[i][3]:\n",
        "    if element.startswith(\"O\"):\n",
        "      count+=1\n",
        "  q[i] = len(words) - count\n",
        "hq = heapq.nlargest(2, q)\n",
        "\n",
        "\n",
        "print(\"Number of parsed candidates \",len(parsed))\n",
        "print(q, heapq.nlargest(2, q)) # ì²«ë²ˆì§¸ ê²½ìš° ì‚¬ìš©\n",
        "\n",
        "\n",
        "words = parsed[0][0]\n",
        "roles = parsed[0][2]\n",
        "tagged = parsed[0][3]\n",
        "for idx in range(len(tagged)):\n",
        "  try:\n",
        "    roles[i] = tagged[i].split(\"-\")[1]\n",
        "  except:\n",
        "    roles[i] = tagged[i].split(\"-\")[0]\n",
        "\n",
        "print(\"roles \",roles)\n",
        "\n",
        "goals = []\n",
        "means = []\n",
        "for _ in range(len(words)):\n",
        "  if roles[_]==\"Goal\":\n",
        "    goals.append(words[_])\n",
        "  elif roles[_]=='Means':\n",
        "    means.append(words[_])\n",
        "  elif roles[_]==\"Purpose\":\n",
        "    purpose.append(words[_])\n",
        "  elif roles[_]==\"Instrument\":\n",
        "    instrument.append(words[_])\n",
        "  elif roles[_]!=\"_\": # using í¬í•¨\n",
        "    instrument.append(words[_])\n",
        "\n",
        "# MEANS\n",
        "MEANS = ' '.join(means)\n",
        "pos = h.pos(MEANS)\n",
        "imp = []\n",
        "for tup in h.pos(MEANS):\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "MEANS_TRUNCATED = ' '.join(imp)\n",
        "print(MEANS_TRUNCATED)\n",
        "\n",
        "# GOALS\n",
        "GOAL = ' '.join(goals)\n",
        "pos = h.pos(GOAL)\n",
        "imp = []\n",
        "for tup in h.pos(GOAL):\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "GOAL_TRUNCATED = ' '.join(imp)\n",
        "print(GOAL_TRUNCATED)\n",
        "\n",
        "final_string = MEANS_TRUNCATED + ' -- >' + GOAL_TRUNCATED\n",
        "print(\"ğŸ˜€ final string to be inserted\",final_string)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}