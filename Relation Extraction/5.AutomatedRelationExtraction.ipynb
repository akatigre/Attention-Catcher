{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutomatedRelationExtraction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoonkim313/dataCampusProject-Team10/blob/master/Relation%20Extraction/AutomatedRelationExtraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "obGBWFLnIypX",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "08999495-273c-429d-ae47-4660184937b4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install transformers\n",
        "!apt-get update\n",
        "!apt-get install g++ openjdk-8-jdk \n",
        "!pip3 install konlpy\n",
        "!pip install kss\n",
        "%cd /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
        "!pip install python-pptx\n",
        "from frameBERT import frame_parser\n",
        "path=\"/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 778kB 3.5MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 12.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.0MB 23.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 29.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=00e03b42e3ddae0e8fcd988ac4b4bac61009becae1e0ef3c448d01a01aeb1283\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:11 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,037 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [882 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Ign:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [255 kB]\n",
            "Get:17 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,855 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [116 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,334 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,413 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [27.1 kB]\n",
            "Get:22 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [895 kB]\n",
            "Fetched 8,088 kB in 3s (2,322 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "g++ is already the newest version (4:7.4.0-1ubuntu2.3).\n",
            "g++ set to manually installed.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxxf86dga1 openjdk-8-jdk-headless openjdk-8-jre\n",
            "  openjdk-8-jre-headless x11-utils\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source visualvm icedtea-8-plugin libnss-mdns\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxxf86dga1 openjdk-8-jdk openjdk-8-jdk-headless\n",
            "  openjdk-8-jre openjdk-8-jre-headless x11-utils\n",
            "0 upgraded, 10 newly installed, 0 to remove and 50 not upgraded.\n",
            "Need to get 40.7 MB of archives.\n",
            "After this operation, 153 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-dejavu-core all 2.37-1 [1,041 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-dejavu-extra all 2.37-1 [1,953 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libatk-wrapper-java all 0.33.3-20ubuntu0.1 [34.7 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libatk-wrapper-java-jni amd64 0.33.3-20ubuntu0.1 [28.3 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jre-headless amd64 8u265-b01-0ubuntu2~18.04 [27.5 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jre amd64 8u265-b01-0ubuntu2~18.04 [69.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jdk-headless amd64 8u265-b01-0ubuntu2~18.04 [8,262 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jdk amd64 8u265-b01-0ubuntu2~18.04 [1,610 kB]\n",
            "Fetched 40.7 MB in 2s (18.0 MB/s)\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../1-fonts-dejavu-core_2.37-1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../2-fonts-dejavu-extra_2.37-1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-1) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../3-x11-utils_7.7+3build1_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+3build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../4-libatk-wrapper-java_0.33.3-20ubuntu0.1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.33.3-20ubuntu0.1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../5-libatk-wrapper-java-jni_0.33.3-20ubuntu0.1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.33.3-20ubuntu0.1) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../6-openjdk-8-jre-headless_8u265-b01-0ubuntu2~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u265-b01-0ubuntu2~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jre:amd64.\n",
            "Preparing to unpack .../7-openjdk-8-jre_8u265-b01-0ubuntu2~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre:amd64 (8u265-b01-0ubuntu2~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../8-openjdk-8-jdk-headless_8u265-b01-0ubuntu2~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u265-b01-0ubuntu2~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk:amd64.\n",
            "Preparing to unpack .../9-openjdk-8-jdk_8u265-b01-0ubuntu2~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk:amd64 (8u265-b01-0ubuntu2~18.04) ...\n",
            "Setting up fonts-dejavu-core (2.37-1) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-1) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u265-b01-0ubuntu2~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u265-b01-0ubuntu2~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "Setting up x11-utils (7.7+3build1) ...\n",
            "Setting up libatk-wrapper-java (0.33.3-20ubuntu0.1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.33.3-20ubuntu0.1) ...\n",
            "Setting up openjdk-8-jre:amd64 (8u265-b01-0ubuntu2~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/policytool to provide /usr/bin/policytool (policytool) in auto mode\n",
            "Setting up openjdk-8-jdk:amd64 (8u265-b01-0ubuntu2~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/appletviewer to provide /usr/bin/appletviewer (appletviewer) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19.4MB 1.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92kB 10.0MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/49/725710351d78d26c65337b1e3b322d7b27b34b704535ab56afc0d9ab0ffd/JPype1-1.0.1-cp36-cp36m-manylinux2010_x86_64.whl (3.8MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.8MB 34.3MB/s \n",
            "\u001b[?25hCollecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7c/99d51f80f3b77b107ebae2634108717362c059a41384a1810d13e2429a81/tweepy-3.9.0-py2.py3-none-any.whl\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.2)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: beautifulsoup4, JPype1, tweepy, colorama, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-1.0.1 beautifulsoup4-4.6.0 colorama-0.4.3 konlpy-0.5.2 tweepy-3.9.0\n",
            "Collecting kss\n",
            "  Downloading https://files.pythonhosted.org/packages/fc/bb/4772901b3b934ac204f32a0bd6fc0567871d8378f9bbc7dd5fd5e16c6ee7/kss-1.3.1.tar.gz\n",
            "Building wheels for collected packages: kss\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-1.3.1-cp36-cp36m-linux_x86_64.whl size=251614 sha256=4e0ff3acdab8d3ae74c38bb40b17d8328c966dad0bc031d38fc8596f4d4f0239\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/98/d1/53f75f89925cd95779824778725ee3fa36e7aa55ed26ad54a8\n",
            "Successfully built kss\n",
            "Installing collected packages: kss\n",
            "Successfully installed kss-1.3.1\n",
            "/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
            "Collecting python-pptx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/86/eb979f7b0333ec769041aae36df8b9f1bd8bea5bbad44620663890dce561/python-pptx-0.6.18.tar.gz (8.9MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8.9MB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from python-pptx) (4.2.6)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.6/dist-packages (from python-pptx) (7.0.0)\n",
            "Collecting XlsxWriter>=0.5.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/b9/0f095f47d1d8fdf830cab77907467beb784bc13c583e27844a22aa923934/XlsxWriter-1.3.2-py2.py3-none-any.whl (142kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143kB 36.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: python-pptx\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-pptx: filename=python_pptx-0.6.18-cp36-none-any.whl size=275706 sha256=5f8c77c7e3256f0d663e317df025a58a729230e8fa2d61cd7f8529e36869bc98\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/1f/2c/29acca422b420a0b5210bd2cd7e9669804520d602d2462f20b\n",
            "Successfully built python-pptx\n",
            "Installing collected packages: XlsxWriter, python-pptx\n",
            "Successfully installed XlsxWriter-1.3.2 python-pptx-0.6.18\n",
            "\n",
            "###DEVICE: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ydc905pQlFy1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6d2ccbcf-a63c-4fe9-c749-d3ca1d07c164"
      },
      "source": [
        "!pip install python-pptx"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-pptx in /usr/local/lib/python3.6/dist-packages (0.6.18)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.6/dist-packages (from python-pptx) (7.0.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from python-pptx) (4.2.6)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.6/dist-packages (from python-pptx) (1.3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MbmSXGrwJ585",
        "colab": {}
      },
      "source": [
        "import kss\n",
        "from konlpy.tag import Hannanum\n",
        "h = Hannanum()\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import heapq\n",
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "from operator import itemgetter \n",
        "from pptx import Presentation\n",
        "from pptx.util import Inches\n",
        "from pptx.enum.shapes import MSO_SHAPE\n",
        "from collections import deque\n",
        "from ast import literal_eval\n",
        "from collections import defaultdict"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_v5o0el7I9ur",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0098c710-c8fe-42fa-c79c-01c8600795a3"
      },
      "source": [
        "text ='''\n",
        "ì‹ ì²´ì˜ ì„¸í¬, ì¡°ì§, ì¥ê¸°ê°€ ì†ìƒë˜ì–´ ë” ì´ìƒ ì œ ê¸°ëŠ¥ì„ í•˜ì§€ëª»í•  ë•Œì— ì´ë¥¼ ëŒ€ì²´í•˜ê¸° ìœ„í•´ ì´ì‹ì„ ì‹¤ì‹œí•œë‹¤. \n",
        "ì´ë•Œ ì´ì‹ìœ¼ë¡œ ì˜®ê²¨ ë¶™ì´ëŠ” ì„¸í¬, ì¡°ì§, ì¥ê¸°ë¥¼ ì´ì‹í¸ì´ë¼ í•œë‹¤. ìì‹ ì´ë‚˜ ì¼ë€ì„± ìŒë‘¥ì´ì˜ ì´ì‹í¸ì„ ì´ìš©í•  \n",
        "ìˆ˜ ì—†ë‹¤ë©´ ë‹¤ë¥¸ ì‚¬ëŒì˜ ì´ì‹í¸ìœ¼ë¡œ â€˜ë™ì¢… ì´ì‹â€™ì„ ì‹¤ì‹œí•œë‹¤. \n",
        "'''\n",
        "text = re.sub(\"\\n\",\" \",text)\n",
        "pprint(text)\n",
        "t = '''\n",
        "ê·¸ëŸ°ë° ìš°ë¦¬ì˜ ëª¸ì€ ìì‹ ì˜ ê²ƒì´ ì•„ë‹Œ ë¬¼ì§ˆì´ \n",
        "ì²´ë‚´ë¡œ ìœ ì…ë  ê²½ìš° ë©´ì—­ ë°˜ì‘ì„ ì¼ìœ¼í‚¤ë¯€ë¡œ, ìœ ì „ì \n",
        "ìœ¼ë¡œ ë™ì¼í•˜ì§€ ì•Šì€ ì´ì‹í¸ì— ëŒ€í•´ í•­ìƒ ê±°ë¶€ ë°˜ì‘ì„ ì¼ìœ¼í‚¨ë‹¤.\n",
        "ë©´ì—­ì  ê±°ë¶€ ë°˜ì‘ì€ ë©´ì—­ ì„¸í¬ê°€ í‘œë©´ì— ë°œí˜„í•˜ëŠ” ì£¼ì¡°ì§ì í•©\n",
        "ë³µí•©ì²´(MHC) ë¶„ìì˜ ì°¨ì´ì— ì˜í•´ ìœ ë°œëœë‹¤. ê°œì²´ë§ˆë‹¤ MHCì—\n",
        "ì°¨ì´ê°€ ìˆëŠ”ë° ì„œë¡œ ê°„ì˜ ìœ ì „ì  ê±°ë¦¬ê°€ ë©€ìˆ˜ë¡ MHCì— ì°¨ì´ê°€\n",
        "ì»¤ì ¸ ê±°ë¶€ ë°˜ì‘ì´ ê°•í•´ì§„ë‹¤. ì´ë¥¼ ë§‰ê¸° ìœ„í•´ ë©´ì—­ ì–µì œì œë¥¼ ì‚¬ìš©\n",
        "í•˜ëŠ”ë°, ì´ëŠ” ë©´ì—­ ë°˜ì‘ì„ ì–µì œí•˜ì—¬ ì§ˆë³‘ ê°ì—¼ì˜ ìœ„í—˜ì„±ì„ ë†’ì¸ë‹¤.\n",
        "ì´ì‹ì—ëŠ” ë§ì€ ë¹„ìš©ì´ ì†Œìš”ë  ë¿ë§Œ ì•„ë‹ˆë¼ ì´ì‹ì´ ê°€ëŠ¥í•œ ë™ì¢…\n",
        "ì´ì‹í¸ì˜ ìˆ˜ê°€ ë§¤ìš° ë¶€ì¡±í•˜ê¸° ë•Œë¬¸ì— ì´ë¥¼ ëŒ€ì²´í•˜ëŠ” ë°©ë²•ì´ ê°œë°œ\n",
        "ë˜ê³  ìˆë‹¤. ìš°ì„  ì¸ê³µ ì‹¬ì¥ê³¼ ê°™ì€ â€˜ì „ì ê¸°ê¸° ì¸ê³µ ì¥ê¸°â€™ë¥¼ ì´ìš©\n",
        "í•˜ëŠ” ë°©ë²•ì´ ìˆë‹¤. í•˜ì§€ë§Œ ì´ëŠ” ì¥ê¸°ì˜ ê¸°ëŠ¥ì„ ì¼ì‹œì ìœ¼ë¡œ ëŒ€ì²´\n",
        "í•˜ëŠ” ë° ì‚¬ìš©ë˜ë©°, ì¶”ê°€ ì „ë ¥ ê³µê¸‰ ë° ì •ê¸°ì  ë¶€í’ˆ êµì²´ ë“±ì´\n",
        "ìš”êµ¬ë˜ëŠ” ë‹¨ì ì´ ìˆê³ , ì•„ì§ ì¸ê°„ì˜ ì¥ê¸°ë¥¼ ì™„ì „íˆ ëŒ€ì²´í•  ë§Œí¼\n",
        "ì •êµí•œ ë‹¨ê³„ì— ì´ë¥´ì§€ëŠ” ëª»í–ˆë‹¤.\n",
        "'''"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(' ì‹ ì²´ì˜ ì„¸í¬, ì¡°ì§, ì¥ê¸°ê°€ ì†ìƒë˜ì–´ ë” ì´ìƒ ì œ ê¸°ëŠ¥ì„ í•˜ì§€ëª»í•  ë•Œì— ì´ë¥¼ ëŒ€ì²´í•˜ê¸° ìœ„í•´ ì´ì‹ì„ ì‹¤ì‹œí•œë‹¤.  ì´ë•Œ ì´ì‹ìœ¼ë¡œ ì˜®ê²¨ '\n",
            " 'ë¶™ì´ëŠ” ì„¸í¬, ì¡°ì§, ì¥ê¸°ë¥¼ ì´ì‹í¸ì´ë¼ í•œë‹¤. ìì‹ ì´ë‚˜ ì¼ë€ì„± ìŒë‘¥ì´ì˜ ì´ì‹í¸ì„ ì´ìš©í•   ìˆ˜ ì—†ë‹¤ë©´ ë‹¤ë¥¸ ì‚¬ëŒì˜ ì´ì‹í¸ìœ¼ë¡œ â€˜ë™ì¢… '\n",
            " 'ì´ì‹â€™ì„ ì‹¤ì‹œí•œë‹¤.  ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9Af1flDKI9M8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "3de3afba-e435-4954-f1b0-a3a30615ceb5"
      },
      "source": [
        "parser = frame_parser.FrameParser(model_path=path, language='ko')"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "srl model: framenet\n",
            "language: ko\n",
            "version: 1.2\n",
            "using viterbi: False\n",
            "using masking: True\n",
            "pretrained BERT: bert-base-multilingual-cased\n",
            "using TGT special token: True\n",
            "used dictionary:\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lu2idx.json\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lufrmap.json\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/mul_bio_frargmap.json\n",
            "...loaded model path: /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
            "/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
            "...model is loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuHEQofWAsnA",
        "colab_type": "text"
      },
      "source": [
        "### Linked List Implemented to Show the whole list of sentences and their parsed tags in a consecutive manner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wIph64I5OnLZ",
        "colab": {}
      },
      "source": [
        "def split_into_sentences(object):\n",
        "  ls = kss.split_sentences(object)\n",
        "  return ls\n",
        "\n",
        "def frameParse(text):\n",
        "  parser = frame_parser.FrameParser(model_path=path, language='ko')\n",
        "  parsed = parser.parser(text, sent_id='1', result_format='conll')\n",
        "  return parsed\n",
        "  \n",
        "\n",
        "def sortCandidate(parsed, num_candidates):\n",
        "  for i in range(len(parsed)):\n",
        "    count=0\n",
        "    for element in parsed[i][3]:\n",
        "      if element.startswith(\"O\"):\n",
        "        count+=1\n",
        "  q[i] = len(words) - count\n",
        "  hq = heapq.nlargest(num_candidates, q)\n",
        "  indices.append(q.index(hq[idx]))\n",
        "  # for idx in range(num_candidates):\n",
        "  #   temp = parsed[q.index(hq[idx])]\n",
        "  #   words = temp[0]\n",
        "  #   [[row[i] for row in temp] for i in range(len(words))]\n",
        "  return indices\n",
        "\n",
        "\n",
        "def findConsecutiveBIO(words, tag):\n",
        "  began = False\n",
        "  count = 0\n",
        "  q = deque(words)\n",
        "  for i in range(len(words)):\n",
        "    if tag[i]=='O':\n",
        "      q.popleft()\n",
        "    if tag[i].startswith('B'):\n",
        "      began=True\n",
        "    if began & tag[i].startswith('I'):\n",
        "      count+=1\n",
        "  return q\n",
        "\n",
        "# Linked List êµ¬í˜„í•´ì„œ parsed í›„ë³´êµ°ì— ì‚¬ìš©\n",
        "class Node:\n",
        "    def __init__(self, data):\n",
        "        self.words = data[0]\n",
        "        self.tags = data[3]\n",
        "        self.next = None\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str((self.words, self.tags))\n",
        "\n",
        "class LinkedList:\n",
        "  def __init__(self):\n",
        "      self.head = None\n",
        "\n",
        "  def __repr__(self):\n",
        "      node = self.head\n",
        "      nodes = []\n",
        "      while node is not None:\n",
        "          nodes.append(str(node.tags))\n",
        "          node = node.next\n",
        "      nodes.append(\"None\")\n",
        "      return ' -> '.join(nodes)\n",
        "  \n",
        "class LinkedList:\n",
        "  def __init__(self):\n",
        "      self.head = None\n",
        "\n",
        "  def __repr__(self):\n",
        "      node = self.head\n",
        "      nodes = []\n",
        "      while node is not None:\n",
        "          nodes.append(str(node.tags))\n",
        "          node = node.next\n",
        "      nodes.append(\"None\")\n",
        "      return ' -> '.join(nodes)\n",
        "\n",
        "def extractFrame(text):\n",
        "  ls = split_into_sentences(text)\n",
        "  final = {}\n",
        "  \n",
        "  for idx in range(len(ls)):\n",
        "    parsed = frameParse(ls[idx]) # candidates ìƒì„±\n",
        "    final.setdefault(idx,str)\n",
        "    parsedList =  LinkedList()\n",
        "    for j in range(len(parsed)):\n",
        "      parsed_candidate = parsed[j]\n",
        "      new_node = Node(parsed_candidate)\n",
        "      if j == 0:\n",
        "        old_node = new_node\n",
        "        parsedList.head = old_node\n",
        "      elif j==len(parsed)-1:\n",
        "        old_node.next = new_node\n",
        "        new_node.next = None\n",
        "        print(idx,'  ',parsedList)\n",
        "        final[idx] = parsedList\n",
        "      else:\n",
        "        old_node.next = new_node\n",
        "        old_node = new_node\n",
        "  return final\n",
        "\n"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD2QZjRqZaM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(text):\n",
        "  all = extractFrame(text)\n",
        "  s = \"\"\n",
        "  res = {}\n",
        "\n",
        "  i = 1\n",
        "  res.setdefault(i,tuple)\n",
        "  for k, v in all.items():\n",
        "    a = v.head\n",
        "    while a is not None:\n",
        "      temp = \" \"\n",
        "      if len(a.words) == len(a.tags):\n",
        "        q = findConsecutiveBIO(a.words, a.tags)\n",
        "      else:\n",
        "        print(\"ERROR OCCURED\")\n",
        "        \n",
        "      count = len(a.words)\n",
        "      for tag in a.tags:\n",
        "       if tag.startswith(\"O\"):\n",
        "          count -= 1\n",
        "      if count > len(a.words)*0.5:\n",
        "        print(\"The threshold is reached !! ğŸ˜› ğŸ˜ ğŸ˜œ ğŸ¤ª ğŸ¤¨ ğŸ§ ğŸ¤“ ğŸ˜ ğŸ¤© ğŸ¥³ ğŸ˜\")\n",
        "        temp = \" \".join(q) \n",
        "        print(temp)\n",
        "        words = a.words\n",
        "        tags = a.tags\n",
        "      res[i] = (a.words, a.tags , \" \".join(q))\n",
        "      a = a.next\n",
        "    i+=1\n",
        "  return res\n"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIIiD0vrpdmD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "94d6d5dd-1a4c-43b5-d646-960fb1ef00f3"
      },
      "source": [
        "extracted = main(text)\n",
        "pprint(extracted)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "srl model: framenet\n",
            "language: ko\n",
            "version: 1.2\n",
            "using viterbi: False\n",
            "using masking: True\n",
            "pretrained BERT: bert-base-multilingual-cased\n",
            "using TGT special token: True\n",
            "used dictionary:\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lu2idx.json\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lufrmap.json\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/mul_bio_frargmap.json\n",
            "...loaded model path: /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
            "/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
            "...model is loaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/utils.py:279: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  pred_logits = sm(masked_logit).view(1,-1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0    ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> ['O', 'O', 'O', 'O', 'O', 'O', 'I-Class', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Time', 'O', 'O', 'O', 'O', 'O'] -> ['O', 'O', 'O', 'I-Landmark_event', 'I-Landmark_event', 'I-Landmark_event', 'I-Landmark_event', 'I-Landmark_event', 'I-Landmark_event', 'I-Landmark_event', 'O', 'O', 'I-Trajector_event', 'I-Trajector_event', 'O', 'I-Trajector_event'] -> ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> ['O', 'O', 'O', 'I-Time', 'I-Time', 'I-Time', 'I-Time', 'I-Time', 'I-Time', 'I-Time', 'I-Time', 'B-Old', 'O', 'O', 'O', 'O'] -> ['O', 'O', 'I-Means', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Time', 'O', 'I-Goal', 'I-Goal', 'O', 'O', 'O'] -> None\n",
            "srl model: framenet\n",
            "language: ko\n",
            "version: 1.2\n",
            "using viterbi: False\n",
            "using masking: True\n",
            "pretrained BERT: bert-base-multilingual-cased\n",
            "using TGT special token: True\n",
            "used dictionary:\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lu2idx.json\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lufrmap.json\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/mul_bio_frargmap.json\n",
            "...loaded model path: /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
            "/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
            "...model is loaded\n",
            "1    ['B-Time', 'B-Theme', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> ['B-Time', 'B-Goal', 'I-Goal', 'O', 'B-Items', 'O', 'O', 'O', 'O'] -> None\n",
            "srl model: framenet\n",
            "language: ko\n",
            "version: 1.2\n",
            "using viterbi: False\n",
            "using masking: True\n",
            "pretrained BERT: bert-base-multilingual-cased\n",
            "using TGT special token: True\n",
            "used dictionary:\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lu2idx.json\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lufrmap.json\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/mul_bio_frargmap.json\n",
            "...loaded model path: /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
            "/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
            "...model is loaded\n",
            "2    ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> ['B-Agent', 'I-Instrument', 'I-Instrument', 'I-Instrument', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> ['B-Entity', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> ['B-Entity', 'I-Entity', 'I-Entity', 'I-Entity', 'I-Entity', 'I-Entity', 'I-Entity', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> None\n",
            "The threshold is reached !! ğŸ˜› ğŸ˜ ğŸ˜œ ğŸ¤ª ğŸ¤¨ ğŸ§ ğŸ¤“ ğŸ˜ ğŸ¤© ğŸ¥³ ğŸ˜\n",
            "ì´ìƒ ì œ ê¸°ëŠ¥ì„ í•˜ì§€ëª»í•  ë•Œì— ì´ë¥¼ ëŒ€ì²´í•˜ê¸° ìœ„í•´ ì´ì‹ì„ ì‹¤ì‹œí•œë‹¤.\n",
            "The threshold is reached !! ğŸ˜› ğŸ˜ ğŸ˜œ ğŸ¤ª ğŸ¤¨ ğŸ§ ğŸ¤“ ğŸ˜ ğŸ¤© ğŸ¥³ ğŸ˜\n",
            "ì œ ê¸°ëŠ¥ì„ í•˜ì§€ëª»í•  ë•Œì— ì´ë¥¼ ëŒ€ì²´í•˜ê¸° ìœ„í•´ ì´ì‹ì„ ì‹¤ì‹œí•œë‹¤.\n",
            "ERROR OCCURED\n",
            "{1: (['ì‹ ì²´ì˜',\n",
            "      'ì„¸í¬,',\n",
            "      'ì¡°ì§,',\n",
            "      'ì¥ê¸°ê°€',\n",
            "      'ì†ìƒë˜ì–´',\n",
            "      'ë”',\n",
            "      'ì´ìƒ',\n",
            "      'ì œ',\n",
            "      'ê¸°ëŠ¥ì„',\n",
            "      'í•˜ì§€ëª»í• ',\n",
            "      'ë•Œì—',\n",
            "      'ì´ë¥¼',\n",
            "      'ëŒ€ì²´í•˜ê¸°',\n",
            "      'ìœ„í•´',\n",
            "      'ì´ì‹ì„',\n",
            "      'ì‹¤ì‹œí•œë‹¤.'],\n",
            "     ['O',\n",
            "      'O',\n",
            "      'I-Means',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O',\n",
            "      'I-Time',\n",
            "      'O',\n",
            "      'I-Goal',\n",
            "      'I-Goal',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O'],\n",
            "     'ëŒ€ì²´í•˜ê¸° ìœ„í•´ ì´ì‹ì„ ì‹¤ì‹œí•œë‹¤.'),\n",
            " 2: (['ì´ë•Œ', 'ì´ì‹ìœ¼ë¡œ', 'ì˜®ê²¨', 'ë¶™ì´ëŠ”', 'ì„¸í¬,', 'ì¡°ì§,', 'ì¥ê¸°ë¥¼', 'ì´ì‹í¸ì´ë¼', 'í•œë‹¤.'],\n",
            "     ['B-Time', 'B-Goal', 'I-Goal', 'O', 'B-Items', 'O', 'O', 'O', 'O'],\n",
            "     'ì¡°ì§, ì¥ê¸°ë¥¼ ì´ì‹í¸ì´ë¼ í•œë‹¤.'),\n",
            " 3: (['ìì‹ ì´ë‚˜',\n",
            "      'ì¼ë€ì„±',\n",
            "      'ìŒë‘¥ì´ì˜',\n",
            "      'ì´ì‹í¸ì„',\n",
            "      'ì´ìš©í• ',\n",
            "      '',\n",
            "      'ìˆ˜',\n",
            "      'ì—†ë‹¤ë©´',\n",
            "      'ë‹¤ë¥¸',\n",
            "      'ì‚¬ëŒì˜',\n",
            "      'ì´ì‹í¸ìœ¼ë¡œ',\n",
            "      'â€˜ë™ì¢…',\n",
            "      'ì´ì‹â€™ì„',\n",
            "      'ì‹¤ì‹œí•œë‹¤.'],\n",
            "     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
            "     '')}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vvm4aMtEz1A1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "6f28df78-5b42-4d60-a3e2-d78b92ad9c89"
      },
      "source": [
        "pprint(extracted)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: (['ì‹ ì²´ì˜',\n",
            "      'ì„¸í¬,',\n",
            "      'ì¡°ì§,',\n",
            "      'ì¥ê¸°ê°€',\n",
            "      'ì†ìƒë˜ì–´',\n",
            "      'ë”',\n",
            "      'ì´ìƒ',\n",
            "      'ì œ',\n",
            "      'ê¸°ëŠ¥ì„',\n",
            "      'í•˜ì§€ëª»í• ',\n",
            "      'ë•Œì—',\n",
            "      'ì´ë¥¼',\n",
            "      'ëŒ€ì²´í•˜ê¸°',\n",
            "      'ìœ„í•´',\n",
            "      'ì´ì‹ì„',\n",
            "      'ì‹¤ì‹œí•œë‹¤.'],\n",
            "     ['O',\n",
            "      'O',\n",
            "      'I-Means',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O',\n",
            "      'I-Time',\n",
            "      'O',\n",
            "      'I-Goal',\n",
            "      'I-Goal',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O'],\n",
            "     ' '),\n",
            " 2: (['ì´ë•Œ', 'ì´ì‹ìœ¼ë¡œ', 'ì˜®ê²¨', 'ë¶™ì´ëŠ”', 'ì„¸í¬,', 'ì¡°ì§,', 'ì¥ê¸°ë¥¼', 'ì´ì‹í¸ì´ë¼', 'í•œë‹¤.'],\n",
            "     ['B-Time', 'B-Goal', 'I-Goal', 'O', 'B-Items', 'O', 'O', 'O', 'O'],\n",
            "     ' '),\n",
            " 3: (['ìì‹ ì´ë‚˜',\n",
            "      'ì¼ë€ì„±',\n",
            "      'ìŒë‘¥ì´ì˜',\n",
            "      'ì´ì‹í¸ì„',\n",
            "      'ì´ìš©í• ',\n",
            "      '',\n",
            "      'ìˆ˜',\n",
            "      'ì—†ë‹¤ë©´',\n",
            "      'ë‹¤ë¥¸',\n",
            "      'ì‚¬ëŒì˜',\n",
            "      'ì´ì‹í¸ìœ¼ë¡œ',\n",
            "      'â€˜ë™ì¢…',\n",
            "      'ì´ì‹â€™ì„',\n",
            "      'ì‹¤ì‹œí•œë‹¤.'],\n",
            "     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
            "     ' ')}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQNkF3aGty_0",
        "colab_type": "text"
      },
      "source": [
        "### TODO : ì—¬ê¸°ì„œë¶€í„° extractedë¥¼ ê°€ì§€ê³  purpose / goal / effect / entity ë“±ì„ ì°¾ì•„ì•¼í•¨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf62cktfw2C1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "f1fddff6-8b96-4d85-83fe-5dbbf2a9ebca"
      },
      "source": [
        "for k, tup in extracted.items():\n",
        "  a, b, c = tup\n",
        "  roles = [0]*len(b)\n",
        "  words = [0]*len(b)\n",
        "  for i in range(len(b)):\n",
        "    try: \n",
        "      roles[i] = b[i].split(\"-\")[1] \n",
        "      \n",
        "    except: \n",
        "      roles[i] = b[i].split(\"-\")[0]\n",
        "  print(roles)\n",
        "  for _ in roles:\n",
        "    pass"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Goal', 'Goal', 'O', 'O', 'Means', 'Means', 'Means']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-bf643a255534>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mroles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'None'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ast.py\u001b[0m in \u001b[0;36mliteral_eval\u001b[0;34m(node_or_string)\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mleft\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'malformed node or string: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ast.py\u001b[0m in \u001b[0;36m_convert\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mleft\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'malformed node or string: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: malformed node or string: <_ast.Name object at 0x7f5ab7354da0>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S7zGTQO4BUO7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "326d4f22-e8b0-4f94-f2dd-23251248d598"
      },
      "source": [
        "# 1st sentence : GOALS & MEANS \n",
        "\n",
        "parsed = parser.parser(ls[0], sent_id='1', result_format='conll')\n",
        "words = parsed[0][0]\n",
        "print(\"ğŸ˜€ Words vector from sentence \",words)\n",
        "q = [-1]*len(parsed)\n",
        "for i in range(len(parsed)):\n",
        "  count=0\n",
        "  for element in parsed[i][3]:\n",
        "    if element.startswith(\"O\"):\n",
        "      count+=1\n",
        "  q[i] = len(words) - count\n",
        "hq = heapq.nlargest(2, q)\n",
        "\n",
        "\n",
        "print(\"Number of parsed candidates \",len(parsed))\n",
        "print(q, heapq.nlargest(2, q)) # ì²«ë²ˆì§¸ ê²½ìš° ì‚¬ìš©\n",
        "\n",
        "\n",
        "words = parsed[0][0]\n",
        "roles = parsed[0][2]\n",
        "tagged = parsed[0][3]\n",
        "for idx in range(len(tagged)):\n",
        "  try:\n",
        "    roles[i] = tagged[i].split(\"-\")[1]\n",
        "  except:\n",
        "    roles[i] = tagged[i].split(\"-\")[0]\n",
        "\n",
        "print(\"roles \",roles)\n",
        "\n",
        "goals = []\n",
        "means = []\n",
        "for _ in range(len(words)):\n",
        "  if roles[_]==\"Goal\":\n",
        "    goals.append(words[_])\n",
        "  if roles[_]=='Means':\n",
        "    means.append(words[_])\n",
        "\n",
        "# MEANS\n",
        "MEANS = ' '.join(means)\n",
        "pos = h.pos(MEANS)\n",
        "imp = []\n",
        "for tup in h.pos(MEANS):\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "MEANS_TRUNCATED = ' '.join(imp)\n",
        "print(MEANS_TRUNCATED)\n",
        "\n",
        "# GOALS\n",
        "GOAL = ' '.join(goals)\n",
        "pos = h.pos(GOAL)\n",
        "imp = []\n",
        "for tup in h.pos(GOAL):\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "GOAL_TRUNCATED = ' '.join(imp)\n",
        "print(GOAL_TRUNCATED)\n",
        "\n",
        "final_string = MEANS_TRUNCATED + ' -- >' + GOAL_TRUNCATED\n",
        "print(\"ğŸ˜€ final string to be inserted\",final_string)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['TEXT2PPTXì˜', 'êµ¬í˜„í•˜ê¸°', 'ìœ„í•´', 'two', 'track', 'processë¥¼', 'ê±°ì³¤ìŠµë‹ˆë‹¤.'], ['_', '_', 'ìœ„í•˜ë‹¤.v', '_', '_', '_', '_'], ['_', '_', 'Purpose', '_', '_', '_', '_'], ['B-Goal', 'I-Goal', 'O', 'O', 'I-Means', 'I-Means', 'I-Means']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lRYgqOwnM5X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "12fd4e08-6821-4b06-9e21-c00404bdbf07"
      },
      "source": [
        ""
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['O', 'O', 'O', 'B-Event', 'I-Event', 'I-Event', 'O']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F9zsJryuBRri",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "dc2971b6-b2a7-444a-bcbe-9f3f7bef79f0"
      },
      "source": [
        "# 2nd sentence : PURPOSE & INSTRUMENT\n",
        "parsed = parser.parser(ls[1], sent_id='1', result_format='conll')\n",
        "words = parsed[0][0]\n",
        "print(\"ğŸ˜€ Words vector from sentence \",words)\n",
        "q = [-1]*len(parsed)\n",
        "for i in range(len(parsed)):\n",
        "  count=0\n",
        "  for element in parsed[i][3]:\n",
        "    if element.startswith(\"O\"):\n",
        "      count+=1\n",
        "  q[i] = len(words) - count\n",
        "\n",
        "\n",
        "print(\"Number of parsed candidates \",len(parsed))\n",
        "print(q, heapq.nlargest(3, q))\n",
        "print(itemgetter(*[1,5])(parsed))\n",
        "candidates = itemgetter(*[1,8])(parsed)\n",
        "for _ in candidates:\n",
        "  words = _[0]\n",
        "  tagged = _[3]\n",
        "  print(\"tagged vector of candidates: \",tagged)\n",
        "  roles = _[2]\n",
        "  for i in range(len(parsed)):\n",
        "    try:\n",
        "      roles[i] = tagged[i].split(\"-\")[1]\n",
        "    except:\n",
        "      roles[i] = tagged[i].split(\"-\")[0]\n",
        "  print(roles,f'Tagged words for the roles : {tagged}')\n",
        "# roles are selected as the one with purpose and instrument\n",
        "\n",
        "purpose = []\n",
        "instrument = []\n",
        "for _ in range(len(words)):\n",
        "  if roles[_]==\"Purpose\":\n",
        "    purpose.append(words[_])\n",
        "    \n",
        "  elif roles[_]==\"Instrument\":\n",
        "    instrument.append(words[_])\n",
        "    \n",
        "  elif roles[_]!=\"_\": # using í¬í•¨\n",
        "    instrument.append(words[_])\n",
        "print(\"Instrument: \",instrument)\n",
        "print(\"Purpose: \",purpose)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# PURPOSE\n",
        "PURPOSE = ' '.join(purpose)\n",
        "pos = h.pos(PURPOSE)\n",
        "imp = []\n",
        "for tup in pos:\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "PURPOSE_TRUNCATED = ' '.join(imp)\n",
        "print(PURPOSE_TRUNCATED)\n",
        "\n",
        "# INSTRUMENT\n",
        "INSTRUMENT = ' '.join(instrument)\n",
        "pos = h.pos(INSTRUMENT)\n",
        "imp = []\n",
        "for tup in pos:\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "INST_TRUNCATED = ' '.join(imp)\n",
        "print(INST_TRUNCATED)\n",
        "\n",
        "final_string = INST_TRUNCATED+ ' â¡ï¸' + PURPOSE_TRUNCATED \n",
        "print(\"ğŸ˜€ final string to be inserted \", final_string)\n",
        "\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/utils.py:279: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  pred_logits = sm(masked_logit).view(1,-1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ğŸ˜€ Words vector from sentence  ['ëŒ€ë³¸ì„', 'í”¼í”¼í‹°ë¡œ', 'ì˜®ê¸°ê¸°', 'ì „ì—', 'ìš”ì•½í•˜ê³ ', 'ë¶„ì„í•˜ê¸°', 'ìœ„í•´', 'ìì—°ì–´ì²˜ë¦¬ì˜', 'ìµœì‹ ', 'ê¸°ìˆ ì„', 'ë‹¤ìˆ˜', 'ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.']\n",
            "Number of parsed candidates  9\n",
            "[2, 11, 4, 0, 6, 1, 1, 1, 11] [11, 11, 6]\n",
            "([['ëŒ€ë³¸ì„', 'í”¼í”¼í‹°ë¡œ', 'ì˜®ê¸°ê¸°', 'ì „ì—', 'ìš”ì•½í•˜ê³ ', 'ë¶„ì„í•˜ê¸°', 'ìœ„í•´', 'ìì—°ì–´ì²˜ë¦¬ì˜', 'ìµœì‹ ', 'ê¸°ìˆ ì„', 'ë‹¤ìˆ˜', 'ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.'], ['_', '_', '_', 'ì „.n', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', 'Time_vector', '_', '_', '_', '_', '_', '_', '_', '_'], ['B-Landmark_event', 'I-Landmark_event', 'I-Landmark_event', 'O', 'B-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event']], [['ëŒ€ë³¸ì„', 'í”¼í”¼í‹°ë¡œ', 'ì˜®ê¸°ê¸°', 'ì „ì—', 'ìš”ì•½í•˜ê³ ', 'ë¶„ì„í•˜ê¸°', 'ìœ„í•´', 'ìì—°ì–´ì²˜ë¦¬ì˜', 'ìµœì‹ ', 'ê¸°ìˆ ì„', 'ë‹¤ìˆ˜', 'ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.'], ['_', '_', '_', '_', '_', '_', '_', '_', 'ìµœì‹ .n', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', 'Relative_time', '_', '_', '_'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Focal_participant', 'O', 'O']])\n",
            "tagged vector of candidates:  ['B-Landmark_event', 'I-Landmark_event', 'I-Landmark_event', 'O', 'B-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event']\n",
            "['Landmark_event', 'Landmark_event', 'Landmark_event', 'O', 'Event', 'Event', 'Event', 'Event', 'Event', '_', '_', '_'] Tagged words for the roles : ['B-Landmark_event', 'I-Landmark_event', 'I-Landmark_event', 'O', 'B-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event']\n",
            "tagged vector of candidates:  ['B-Purpose', 'I-Purpose', 'I-Purpose', 'I-Purpose', 'I-Purpose', 'I-Purpose', 'I-Purpose', 'B-Instrument', 'I-Instrument', 'I-Instrument', 'B-Manner', 'O']\n",
            "['Purpose', 'Purpose', 'Purpose', 'Purpose', 'Purpose', 'Purpose', 'Purpose', 'Instrument', 'Instrument', '_', '_', 'Using'] Tagged words for the roles : ['B-Purpose', 'I-Purpose', 'I-Purpose', 'I-Purpose', 'I-Purpose', 'I-Purpose', 'I-Purpose', 'B-Instrument', 'I-Instrument', 'I-Instrument', 'B-Manner', 'O']\n",
            "Instrument:  ['ìì—°ì–´ì²˜ë¦¬ì˜', 'ìµœì‹ ', 'ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.']\n",
            "Purpose:  ['ëŒ€ë³¸ì„', 'í”¼í”¼í‹°ë¡œ', 'ì˜®ê¸°ê¸°', 'ì „ì—', 'ìš”ì•½í•˜ê³ ', 'ë¶„ì„í•˜ê¸°', 'ìœ„í•´']\n",
            "ëŒ€ë³¸ í”¼í”¼í‹° ì „ ìš”ì•½ ë¶„ì„\n",
            "ìì—°ì–´ì²˜ë¦¬ ìµœì‹  ì‚¬ìš©\n",
            "ğŸ˜€ final string to be inserted  ìì—°ì–´ì²˜ë¦¬ ìµœì‹  ì‚¬ìš© â¡ï¸ëŒ€ë³¸ í”¼í”¼í‹° ì „ ìš”ì•½ ë¶„ì„\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HxZxxnMiVuKw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "945f2131-26cd-44d3-cd1c-c00ef0bb5f39"
      },
      "source": [
        "# 3rd sentence : Goal - Means\n",
        "parsed = parser.parser(ls[2], sent_id='1', result_format='conll')\n",
        "words = parsed[0][0]\n",
        "print(\"Words vector from sentence \",words)\n",
        "q = [-1]*len(parsed)\n",
        "for i in range(len(parsed)):\n",
        "  count=0\n",
        "  for element in parsed[i][3]:\n",
        "    if element.startswith(\"O\"):\n",
        "      count+=1\n",
        "  q[i] = len(words) - count\n",
        "\n",
        "print(q, heapq.nlargest(3, q))\n",
        "\n",
        "a,b,c=parsed\n",
        "tagged = b[3]\n",
        "for i in range(len(tagged)):\n",
        "  try:\n",
        "    roles[i] = tagged[i].split(\"-\")[1]\n",
        "  except:\n",
        "    roles[i] = tagged[i].split(\"-\")[0]\n",
        "\n",
        "\n",
        "\n",
        "goals = []\n",
        "means = []\n",
        "for _ in range(len(words)):\n",
        "  if roles[_]==\"Goal\":\n",
        "    goals.append(words[_])\n",
        "  if roles[_]=='Means':\n",
        "    means.append(words[_])\n",
        "\n",
        "# MEANS\n",
        "MEANS = ' '.join(means)\n",
        "pos = h.pos(MEANS)\n",
        "imp = []\n",
        "for tup in h.pos(MEANS):\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "MEANS_TRUNCATED = ' '.join(imp)\n",
        "print(MEANS_TRUNCATED)\n",
        "\n",
        "# GOALS\n",
        "GOAL = ' '.join(goals)\n",
        "pos = h.pos(GOAL)\n",
        "imp = []\n",
        "for tup in h.pos(GOAL):\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "GOAL_TRUNCATED = ' '.join(imp)\n",
        "print(GOAL_TRUNCATED)\n",
        "\n",
        "final_string = MEANS_TRUNCATED + ' -- >' + GOAL_TRUNCATED\n",
        "print(\"ğŸ˜€ final string to be inserted\",final_string)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/utils.py:279: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  pred_logits = sm(masked_logit).view(1,-1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Words vector from sentence  ['ë˜í•œ', 'íŒŒì´ì¬ì—ì„œ', 'íŒŒì›Œí¬ì¸íŠ¸ì˜', 'ì†ŒìŠ¤ì—', 'ì ‘ê·¼í•˜ê¸°', 'ìœ„í•´', 'xml', 'ì½”ë“œë¥¼', 'ì‹¬ì¸µì ìœ¼ë¡œ', 'ë¶„ì„í–ˆìŠµë‹ˆë‹¤.']\n",
            "[2, 8, 3] [8, 3, 2]\n",
            "askdf\n",
            "askdf\n",
            "askdf\n",
            "askdf\n",
            "   \n",
            "ì½”ë“œ ì‹¬ì¸µì  ë¶„ì„\n",
            "íŒŒì´ì¬ íŒŒì›Œí¬ì¸íŠ¸ ì†ŒìŠ¤ ì ‘ê·¼í•˜ê¸°\n",
            "ğŸ˜€ final string to be inserted ì½”ë“œ ì‹¬ì¸µì  ë¶„ì„-->íŒŒì´ì¬ íŒŒì›Œí¬ì¸íŠ¸ ì†ŒìŠ¤ ì ‘ê·¼í•˜ê¸°\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VQxErkIzOj0W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "8cbbe6fe-11dd-4e8b-a633-1aff0b26911a"
      },
      "source": [
        "# # Fourth Sentence\n",
        "\n",
        "# parsed = parser.parser(ls[3], sent_id='1', result_format='conll')\n",
        "# words = parsed[0][0]\n",
        "# print(\"ğŸ˜€ Words vector from sentence \",words)\n",
        "# q = [-1]*len(parsed)\n",
        "# for i in range(len(parsed)):\n",
        "#   count=0\n",
        "#   for element in parsed[i][3]:\n",
        "#     if element.startswith(\"O\"):\n",
        "#       count+=1\n",
        "#   q[i] = len(words) - count\n",
        "\n",
        "\n",
        "# print(\"Number of parsed candidates \",len(parsed))\n",
        "# print(q, heapq.nlargest(3, q))\n",
        "\n",
        "\n",
        "candidates = itemgetter(*[0,7])(parsed)\n",
        "for _ in candidates:\n",
        "  words = _[0]\n",
        "  tagged = _[3]\n",
        "  print(\"tagged vector of candidates: \",tagged)\n",
        "  roles = _[2]\n",
        "  for i in range(len(tagged)):\n",
        "    try:\n",
        "      roles[i] = tagged[i].split(\"-\")[1]\n",
        "    except:\n",
        "      roles[i] = tagged[i].split(\"-\")[0]\n",
        "\n",
        "  print(words,f'\\nTagged words for the roles : ',roles)\n",
        "  print()\n",
        "# roles are selected as the one with effects only\n",
        "\n",
        "tagged = parsed[0][3]\n",
        "roles = parsed[0][2]\n",
        "for i in range(len(tagged)):\n",
        "    try:\n",
        "      roles[i] = tagged[i].split(\"-\")[1]\n",
        "    except:\n",
        "      roles[i] = tagged[i].split(\"-\")[0]\n",
        "\n",
        "print(words,'\\nTagged words for the roles : ',roles)  \n",
        "print()\n",
        "\n",
        "\n",
        "\n",
        "purpose = []\n",
        "instrument = []\n",
        "effects = []\n",
        "entity = []\n",
        "\n",
        "for _ in range(len(words)):\n",
        "  if roles[_]==\"Purpose\":\n",
        "    purpose.append(words[_])\n",
        "    \n",
        "  if roles[_]==\"Instrument\":\n",
        "    instrument.append(words[_])\n",
        "  \n",
        "  if roles[_]==\"Effect\":\n",
        "    effects.append(words[_])\n",
        "    \n",
        "  \n",
        "  if roles[_]==\"Entity\":\n",
        "    entity.append(words[_])\n",
        "    \n",
        "  elif roles[_]!=\"_\": # using í¬í•¨\n",
        "    instrument.append(words[_])\n",
        "print(\"Effects \",effects)\n",
        "\n",
        "\n",
        "# Effects\n",
        "EFFECT = ' '.join(effects)\n",
        "pos = h.pos(EFFECT)\n",
        "imp = []\n",
        "for tup in pos:\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "EFFECT_TRUNCATED = ' '.join(imp)\n",
        "print(EFFECT_TRUNCATED)\n",
        "\n",
        "final_string = EFFECT_TRUNCATED\n",
        "print(\"ğŸ˜€ final string to be inserted \", final_string)\n",
        "\n",
        "    \n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tagged vector of candidates:  ['O', 'O', 'B-Effect', 'I-Effect', 'I-Effect', 'I-Effect', 'I-Effect', 'I-Effect', 'I-Effect', 'I-Effect', 'I-Effect', 'I-Effect', 'I-Effect', 'I-Effect', 'I-Effect']\n",
            "['ê·¸', 'ê²°ê³¼', 'ì‚¬ìš©ìê°€', 'ìì—°ì–´ë¡œ', 'ì“°ì¸', 'ëŒ€ë³¸ì„', 'í…2í”¼ì—', 'ì œê³µí•˜ë©´', 'ë†’ì€', 'ìˆ˜ì¤€ì˜', 'í”¼í”¼í‹°ë¡œ', 'ì œê³µí• ', 'ìˆ˜', 'ìˆê²Œ', 'ë˜ì—ˆìŠµë‹ˆë‹¤.'] \n",
            "Tagged words for the roles :  ['O', 'O', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect']\n",
            "\n",
            "tagged vector of candidates:  ['O', 'O', 'B-Entity', 'I-Entity', 'I-Entity', 'I-Entity', 'I-Entity', 'I-Entity', 'I-Event', 'I-Entity', 'I-Entity', 'I-Entity', 'I-Entity', 'O', 'O']\n",
            "['ê·¸', 'ê²°ê³¼', 'ì‚¬ìš©ìê°€', 'ìì—°ì–´ë¡œ', 'ì“°ì¸', 'ëŒ€ë³¸ì„', 'í…2í”¼ì—', 'ì œê³µí•˜ë©´', 'ë†’ì€', 'ìˆ˜ì¤€ì˜', 'í”¼í”¼í‹°ë¡œ', 'ì œê³µí• ', 'ìˆ˜', 'ìˆê²Œ', 'ë˜ì—ˆìŠµë‹ˆë‹¤.'] \n",
            "Tagged words for the roles :  ['O', 'O', 'Entity', 'Entity', 'Entity', 'Entity', 'Entity', 'Entity', 'Event', 'Entity', 'Entity', 'Entity', 'Entity', 'O', 'O']\n",
            "\n",
            "['ê·¸', 'ê²°ê³¼', 'ì‚¬ìš©ìê°€', 'ìì—°ì–´ë¡œ', 'ì“°ì¸', 'ëŒ€ë³¸ì„', 'í…2í”¼ì—', 'ì œê³µí•˜ë©´', 'ë†’ì€', 'ìˆ˜ì¤€ì˜', 'í”¼í”¼í‹°ë¡œ', 'ì œê³µí• ', 'ìˆ˜', 'ìˆê²Œ', 'ë˜ì—ˆìŠµë‹ˆë‹¤.'] \n",
            "Tagged words for the roles :  ['O', 'O', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect']\n",
            "\n",
            "Effects  ['ì‚¬ìš©ìê°€', 'ìì—°ì–´ë¡œ', 'ì“°ì¸', 'ëŒ€ë³¸ì„', 'í…2í”¼ì—', 'ì œê³µí•˜ë©´', 'ë†’ì€', 'ìˆ˜ì¤€ì˜', 'í”¼í”¼í‹°ë¡œ', 'ì œê³µí• ', 'ìˆ˜', 'ìˆê²Œ', 'ë˜ì—ˆìŠµë‹ˆë‹¤.']\n",
            "ì‚¬ìš©ì ìì—°ì–´ ëŒ€ë³¸ í…2í”¼ ì œê³µ ìˆ˜ì¤€ í”¼í”¼í‹° ì œê³µ ìˆ˜\n",
            "ğŸ˜€ final string to be inserted  ì‚¬ìš©ì ìì—°ì–´ ëŒ€ë³¸ í…2í”¼ ì œê³µ ìˆ˜ì¤€ í”¼í”¼í‹° ì œê³µ ìˆ˜\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89ANwCXZXnXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_vBkRqWXnfr",
        "colab_type": "text"
      },
      "source": [
        "## PPTXì— í™œìš©"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ_738Krj4Wm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prs = Presentation(\"final_template.pptx\")  # ì›í•˜ëŠ” template ì¢…ë¥˜ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "slide_0 = prs.slides[0] # ì œë³µ slide (ì œëª© ìŠ¬ë¼ì´ë“œ)\n",
        "slide_2 = prs.slides[2] # ë‘ ë²ˆì§¸ slide"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ_iciyKj4Wp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "slide_2.placeholders.element[3][2][2][1][1].text = \"Two Track Process\" # ì—¬ëŸ ë²ˆì§¸ ìŠ¬ë¼ì´ë“œ ì œëª©\n",
        "slide_2.placeholders.element[5][2][2][1][1].text = \"\"\n",
        "slide_2.placeholders.element[6][2][2][1][1].text = \"\"\n",
        "slide_2.placeholders.element[17][2][2][1][1].text = \"\"\n",
        "slide_2.placeholders.element[18][2][2][1][1].text = \"\"\n",
        "slide_2.placeholders.element[19][2][2][1][1].text = \"TEXT2PPTX\"\n",
        "slide_2.placeholders.element[11][2][2][1][1].text = \"ìì—°ì–´ì²˜ë¦¬ì˜ ìµœì‹  ê¸°ìˆ ì„ ë‹¤ìˆ˜ ì‚¬ìš©\"\n",
        "slide_2.placeholders.element[10][2][2][1][1].text = \"ëŒ€ë³¸ì„ í”¼í”¼í‹°ë¡œ ì˜®ê¸°ê¸° ì „ì— ìš”ì•½í•˜ê³  ë¶„ì„\"\n",
        "slide_2.placeholders.element[14][2][2][1][1].text = \"íŒŒì´ì¬ì—ì„œ íŒŒì›Œí¬ì¸íŠ¸ì˜ ì†ŒìŠ¤ì— ì ‘ê·¼í•˜ê¸° ìœ„í•´ xml ì½”ë“œë¥¼ ì‹¬ì¸µì ìœ¼ë¡œ ë¶„ì„\"\n",
        "slide_2.placeholders.element[13][2][2][1][1].text = \"ì‚¬ìš©ìê°€ ìì—°ì–´ë¡œ ì“°ì¸ ëŒ€ë³¸ì„ í…2í”¼ì— ì œê³µí•˜ë©´ ë†’ì€ ìˆ˜ì¤€ì˜ í”¼í”¼í‹°ë¡œ ì œê³µ\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEgNzSF4Xq8a",
        "colab_type": "text"
      },
      "source": [
        "## êµìœ¡ìš© êµì¬ì— í™œìš©"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRL1bZb4XusT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
